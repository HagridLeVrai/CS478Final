{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf4ea8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from pathlib2 import Path\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3a2c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image settings\n",
    "batch_size = 32\n",
    "img_height = 200\n",
    "img_width = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "924f10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get database\n",
    "## From Internet\n",
    "dataset_url = \"https://dl.dropboxusercontent.com/s/fz3oksq1zh7iy58/dataset.tar.gz?dl=0\"\n",
    "data_dir = tf.keras.utils.get_file('dataset', origin=dataset_url, untar=True)\n",
    "data_dir = Path(data_dir)\n",
    "\n",
    "## LOCAL\n",
    "#data_dir = Path('./dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e98cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9780\n"
     ]
    }
   ],
   "source": [
    "## Count images\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90fbabfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9780 files belonging to 10 classes.\n",
      "Using 7824 files for training.\n"
     ]
    }
   ],
   "source": [
    "training_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e7974ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9780 files belonging to 10 classes.\n",
      "Using 1956 files for validation.\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37ce881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = training_dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b3f0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "training_dataset = training_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6c888c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "normalized_dataset = training_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_dataset))\n",
    "first_image = image_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36165082",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1029e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "245/245 [==============================] - 132s 536ms/step - loss: 1.6542 - accuracy: 0.4218 - val_loss: 1.3412 - val_accuracy: 0.4954\n",
      "Epoch 2/20\n",
      "245/245 [==============================] - 132s 538ms/step - loss: 1.2396 - accuracy: 0.5382 - val_loss: 1.2158 - val_accuracy: 0.5440\n",
      "Epoch 3/20\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 1.0792 - accuracy: 0.5870 - val_loss: 1.1145 - val_accuracy: 0.5654\n",
      "Epoch 4/20\n",
      "245/245 [==============================] - 133s 543ms/step - loss: 0.9426 - accuracy: 0.6411 - val_loss: 1.1794 - val_accuracy: 0.5680\n",
      "Epoch 5/20\n",
      "245/245 [==============================] - 133s 543ms/step - loss: 0.8117 - accuracy: 0.6934 - val_loss: 1.1323 - val_accuracy: 0.5711\n",
      "Epoch 6/20\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 0.6592 - accuracy: 0.7527 - val_loss: 1.2215 - val_accuracy: 0.5706\n",
      "Epoch 7/20\n",
      "245/245 [==============================] - 134s 546ms/step - loss: 0.4988 - accuracy: 0.8162 - val_loss: 1.4122 - val_accuracy: 0.5670\n",
      "Epoch 8/20\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 0.3684 - accuracy: 0.8649 - val_loss: 1.7033 - val_accuracy: 0.5690\n",
      "Epoch 9/20\n",
      "245/245 [==============================] - 134s 545ms/step - loss: 0.2578 - accuracy: 0.9093 - val_loss: 1.8714 - val_accuracy: 0.5562\n",
      "Epoch 10/20\n",
      "245/245 [==============================] - 134s 546ms/step - loss: 0.1928 - accuracy: 0.9355 - val_loss: 2.1547 - val_accuracy: 0.5629\n",
      "Epoch 11/20\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 0.1526 - accuracy: 0.9486 - val_loss: 2.2811 - val_accuracy: 0.5670\n",
      "Epoch 12/20\n",
      "245/245 [==============================] - 133s 545ms/step - loss: 0.1447 - accuracy: 0.9513 - val_loss: 2.3365 - val_accuracy: 0.5690\n",
      "Epoch 13/20\n",
      "245/245 [==============================] - 133s 545ms/step - loss: 0.1146 - accuracy: 0.9683 - val_loss: 2.3916 - val_accuracy: 0.5629\n",
      "Epoch 14/20\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 0.0939 - accuracy: 0.9734 - val_loss: 2.6466 - val_accuracy: 0.5598\n",
      "Epoch 15/20\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 0.1052 - accuracy: 0.9664 - val_loss: 2.7739 - val_accuracy: 0.5557\n",
      "Epoch 16/20\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 0.0845 - accuracy: 0.9771 - val_loss: 2.7396 - val_accuracy: 0.5496\n",
      "Epoch 17/20\n",
      "245/245 [==============================] - 134s 546ms/step - loss: 0.0760 - accuracy: 0.9748 - val_loss: 2.7095 - val_accuracy: 0.5486\n",
      "Epoch 18/20\n",
      "245/245 [==============================] - 134s 545ms/step - loss: 0.0655 - accuracy: 0.9825 - val_loss: 2.9937 - val_accuracy: 0.5619\n",
      "Epoch 19/20\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 0.0742 - accuracy: 0.9810 - val_loss: 2.9788 - val_accuracy: 0.5557\n",
      "Epoch 20/20\n",
      "245/245 [==============================] - 133s 544ms/step - loss: 0.0630 - accuracy: 0.9840 - val_loss: 2.9894 - val_accuracy: 0.5567\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "epochs=20\n",
    "history = model.fit(\n",
    "  training_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf3ecb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "951c2975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This person is most likely to be 41-50 years old. 97.78 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "## Testing with Dwayne Johnson\n",
    "test_img_path = Path('./test_images/rock.jpg')\n",
    "\n",
    "img = tf.keras.utils.load_img(test_img_path, target_size=(img_height, img_width))\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) \n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This person is most likely to be {} years old. {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ecc62710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This person is most likely to be 11-20 years old. 99.70 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "## Testing with a 20 years old person.\n",
    "test_img_path = Path('./test_images/imane.jpg')\n",
    "\n",
    "img = tf.keras.utils.load_img(test_img_path, target_size=(img_height, img_width))\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) \n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This person is most likely to be {} years old. {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b688b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This person is most likely to be 71-80 years old. 65.92 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "## Testing with Robert Downey JR.\n",
    "test_img_path = Path('./test_images/rdj.jpg')\n",
    "\n",
    "img = tf.keras.utils.load_img(test_img_path, target_size=(img_height, img_width))\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) \n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This person is most likely to be {} years old. {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d51c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
